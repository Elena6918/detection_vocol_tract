{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32062a7a-e928-4584-ae98-dfbeab88e99d",
   "metadata": {},
   "source": [
    "# This notebook will create the df_timit and save it as a csv for later use. \n",
    "    - We need to step through all the directories in the timit data store and create necessary rows\n",
    "    - Also need to augment the data with some additional info\n",
    "    \n",
    "## Dialect information\n",
    "     dr1:  New England\n",
    "     dr2:  Northern\n",
    "     dr3:  North Midland\n",
    "     dr4:  South Midland\n",
    "     dr5:  Southern\n",
    "     dr6:  New York City\n",
    "     dr7:  Western\n",
    "     dr8:  Army Brat (moved around)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7261de6-387e-46cf-ad9b-b6ef0e0517e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "#increase some pandas vars\n",
    "pd.set_option('max_colwidth', 100)\n",
    "sys_name = os.uname()[1]\n",
    "\n",
    "def data_path_train():    \n",
    "    if sys_name == 'Hive':\n",
    "        return \"/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/guesswho_true\"\n",
    "        #return \"/home/logan/drive/Research/Data_Stores/Timit/Data/timit/TIMIT/TRAIN\"\n",
    "    else:\n",
    "        return \"/home/logan/SynologyDrive/Research/guesswho_new/guesswho18/data/deepfake_data/guesswho_true\"\n",
    "        #return \"/Users/logan/Google_Drive/Research/Data_Stores/Timit/Data/timit/TIMIT/TRAIN\"\n",
    "        \n",
    "def data_path_lyre_true_extend(): \n",
    "    return \"/home/logan/SynologyDrive/Research/guesswho_new/guesswho18/data/deepfake_data/lyre_bird_comp/real/\"\n",
    "\n",
    "def data_path_lyre_fake_extend(): \n",
    "    return \"/home/logan/SynologyDrive/Research/guesswho_new/guesswho18/data/deepfake_data/lyre_bird_comp/fake/\"\n",
    "    \n",
    "def data_path_fakes():\n",
    "    if sys_name == 'Hive':\n",
    "        return \"/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/guesswho_fakes/\"\n",
    "    else:\n",
    "        return \"//home/logan/SynologyDrive/Research/guesswho_new/guesswho18/data/deepfake_data/guesswho_fakes/\"\n",
    "\n",
    "def data_path_test():\n",
    "    sys_name = os.uname()[1]\n",
    "    if sys_name == 'Hive':\n",
    "        return \"/home/logan/drive/Research/Data_Stores/Timit/Data/timit/TIMIT/TEST\"\n",
    "    else:\n",
    "        return \"/home/logan/SynologyDrive/Research/Data_Stores/Timit/Data/timit/TIMIT/TEST\"\n",
    "\n",
    "def append_file(root, file, hive=False, add_slash=False):\n",
    "    if hive:\n",
    "        mod_root = \"E:\\\\SynologyDrive\\\\\" + root[root.find('Research'):]\n",
    "        if add_slash:\n",
    "            output = mod_root  + '\\\\' + file\n",
    "        else:\n",
    "            output = mod_root  + file\n",
    "        output =  output.replace('/', '\\\\')\n",
    "    else:\n",
    "        if add_slash:\n",
    "            output = root + '/' + file\n",
    "        else:\n",
    "            output = root + file\n",
    "    return output\n",
    "\n",
    "def cut_speaker_id(path):\n",
    "    return path[path.rfind('/') + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f64561fb-6dac-49d4-ae06-6ff23429df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arpabet to ipa conversion\n",
    "arpa_key = ['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'axr', 'ay', 'eh', 'er', 'ey', 'ih', 'ix', 'iy', \\\n",
    "                'ow', 'oy', 'uh', 'uw', 'ux' , 'b', 'ch', 'd', 'dh', 'dx', 'el', 'em', 'en', 'f', \\\n",
    "                'g', 'h', 'hh', 'jh', 'k', 'l', 'm', 'n', 'ng', 'nx', 'p', 'q', 'r', 's', 'sh', \\\n",
    "                't', 'th', 'v', 'w', 'wh', 'y', 'z', 'zh', 'ax-h', 'bcl', 'dcl', 'eng', 'gcl', 'hv', \\\n",
    "                'kcl', 'pcl', 'tcl', 'pau', 'epi', 'h#']\n",
    "ipa_key = ['ɑ', 'æ', 'ʌ', 'ɔ', 'aʊ', 'ə', 'ɚ', 'aɪ', 'ɛ', 'ɝ', 'eɪ', 'ɪ', 'ɨ', 'i', 'oʊ', 'ɔɪ', \\\n",
    "                'ʊ', 'u', 'ʉ', 'b', 'tʃ', 'd', 'ð', 'ɾ', 'l̩', 'm̩', 'n̩', 'f', 'ɡ', 'h', 'h', 'dʒ', 'k', \\\n",
    "                'l', 'm', 'n', 'ŋ', 'ɾ̃', 'p', 'ʔ', 'ɹ', 's', 'ʃ', 't', 'θ', 'v', 'w', 'ʍ', 'j', 'z', \\\n",
    "                'ʒ', 'ə̥', 'b̚', 'd̚', 'ŋ̍', 'ɡ̚', 'ɦ', 'k̚', 'p̚', 't̚', 'N/A', 'N/A', 'N/A']\n",
    "\n",
    "ipa_conversion = dict(zip(arpa_key, ipa_key))\n",
    "def convert_to_ipa(arpa_key):\n",
    "    output = []\n",
    "    for key in arpa_key:\n",
    "        output.append(ipa_conversion[key])\n",
    "    return output\n",
    "\n",
    "arpa_conversion = dict(zip(ipa_key, arpa_key))\n",
    "def convert_from_ipa(ipa_key):\n",
    "    output = []\n",
    "    for key in ipa_key:\n",
    "        output.append(arpa_conversion.get(key, 'N/A'))\n",
    "    return output\n",
    "\n",
    "def join_word_phoneme(df_wrd, df_phn, audio_file):\n",
    "    new_df = df_wrd.merge(df_phn, 'outer', on=('sample_id', 'speaker_id'), \\\n",
    "                          suffixes=('_word', '_phoneme'))\n",
    "    new_df = new_df[(new_df.start_phoneme >= new_df.start_word) &\n",
    "                    (new_df.end_phoneme <= new_df.end_word)]\n",
    "    try:\n",
    "        new_df['ipa'] = convert_to_ipa(new_df['arpabet'])\n",
    "        new_df['filepath'] = audio_file\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "    if audio_file[audio_file.rfind('/') - 5] == 'M':\n",
    "        new_df['sex'] = 'm'\n",
    "    else:\n",
    "        new_df['sex'] = 'f'\n",
    "    return new_df\n",
    "\n",
    "def ipa_join_word_phoneme(df_wrd, df_phn, audio_file):\n",
    "    new_df = df_wrd.merge(df_phn, 'outer', on=('sample_id', 'speaker_id'), \\\n",
    "                          suffixes=('_word', '_phoneme'))\n",
    "    new_df = new_df[(new_df.start_phoneme >= new_df.start_word) &\n",
    "                    (new_df.end_phoneme <= new_df.end_word)]\n",
    "    try:\n",
    "        new_df['arpabet'] = convert_from_ipa(new_df['ipa'])\n",
    "        new_df['filepath'] = audio_file\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "    if audio_file[audio_file.rfind('/') - 5] == 'M':\n",
    "        new_df['sex'] = 'm'\n",
    "    else:\n",
    "        new_df['sex'] = 'f'\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea2d26-4708-4fca-9b5b-8dce32fb9905",
   "metadata": {},
   "source": [
    "# ASV Spoof\n",
    "\n",
    "## True set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a72312-046a-405b-ab28-082267675bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [1:35:16<00:00, 17.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_test = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', 'start_phoneme', \n",
    "        'end_phoneme', 'sex', 'arpabet', 'ipa', 'filepath'])\n",
    "\n",
    "data_path = '/home/logan/SynologyDrive/Research/Data_Stores/guesswho_fakes/ASV_spoof_usenix/batch_2'\n",
    "for root, dirs, files in tqdm(list(os.walk(data_path)), position=0, leave=True):     #training data\n",
    "    for file in files:\n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            speaker_id = root[-16:-9]            \n",
    "                \n",
    "            #get phoneme infor\n",
    "            phn_file = append_file(root, sample_id+'.PHN', add_slash=True)\n",
    "            df_phn = pd.read_csv(phn_file, delimiter=' ', names=['start', 'end', 'ipa'])\n",
    "            df_phn['sample_id'] = sample_id\n",
    "            df_phn['speaker_id'] = speaker_id\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            df_test = df_test.append(ipa_join_word_phoneme(df_wrd, df_phn, \n",
    "                                    append_file(root, file, hive=True, add_slash=True)), ignore_index=True)\n",
    "\n",
    "#add phoneme indices for bigram analysis\n",
    "df_test['index_phoneme'] = -1\n",
    "grouped_df = df_test.groupby(['speaker_id', 'sample_id', 'start_word'])\n",
    "for key, item in grouped_df:\n",
    "    new_indices = list(range(len(item.index_phoneme)))\n",
    "    df_test.loc[item.index, 'index_phoneme'] = new_indices\n",
    "    \n",
    "#filter out unknown ipa values (epi, silence)\n",
    "all_ph = ['ʃ', 'ɨ', 'ɦ', 'ɛ', 'd̚', 'dʒ', 'ɪ', 'd', 'ʌ', 'k̚', 'k', 's', 'ʉ',\n",
    "       'ʔ', 'n̩', 'ɡ̚', 'ɡ', 'ɹ', 'w', 'ɔ', 'ɾ', 'ɚ', 'l', 'j', 'ʊ', 'n',\n",
    "       'æ', 'm', 'ɔɪ', 'ə', 'ð', 't̚', 'i', 'v', 'f', 't', 'p̚', 'oʊ',\n",
    "       'h', 'tʃ', 'b̚', 'b', 'ɑ', 'm̩', 'ŋ', 'aɪ', 'θ', 'ə̥', 'eɪ',\n",
    "       'p', 'aʊ', 'ɝ', 'ɾ̃', 'z', 'l̩', 'u', 'ʒ', 'ŋ̍']\n",
    "\n",
    "df_test = df_test[df_test.ipa.isin(all_ph)]\n",
    "df_test.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_test.to_csv('../../data/asv_usenix_batch_2_master.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b84892-0311-486c-a28e-4a426ea62de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_3.8.0",
   "language": "python",
   "name": "gen_3.8.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
