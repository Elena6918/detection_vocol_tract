{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# This notebook will create the df_timit and save it as a csv for later use. \n",
    "    - We need to step through all the directories in the timit data store and create necessary rows\n",
    "    - Also need to augment the data with some additional info\n",
    "    \n",
    "## Dialect information\n",
    "     dr1:  New England\n",
    "     dr2:  Northern\n",
    "     dr3:  North Midland\n",
    "     dr4:  South Midland\n",
    "     dr5:  Southern\n",
    "     dr6:  New York City\n",
    "     dr7:  Western\n",
    "     dr8:  Army Brat (moved around)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "#increase some pandas vars\n",
    "pd.set_option('max_colwidth', 100)\n",
    "sys_name = os.uname()[1]\n",
    "\n",
    "def data_path_train():    \n",
    "    if sys_name == 'Hive':\n",
    "        return \"/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/guesswho_true\"\n",
    "        #return \"/home/logan/drive/Research/Data_Stores/Timit/Data/timit/TIMIT/TRAIN\"\n",
    "    else:\n",
    "        return \"/home/logan/SynologyDrive/Research/guesswho_new/guesswho18/data/deepfake_data/guesswho_true\"\n",
    "        #return \"/Users/logan/Google_Drive/Research/Data_Stores/Timit/Data/timit/TIMIT/TRAIN\"\n",
    "        \n",
    "def data_path_lyre_true_extend(): \n",
    "    return \"/home/logan/SynologyDrive/Research/guesswho_new/guesswho18/data/deepfake_data/lyre_bird_comp/real/\"\n",
    "\n",
    "def data_path_lyre_fake_extend(): \n",
    "    return \"/home/logan/SynologyDrive/Research/guesswho_new/guesswho18/data/deepfake_data/lyre_bird_comp/fake/\"\n",
    "    \n",
    "def data_path_fakes():\n",
    "    if sys_name == 'Hive':\n",
    "        return \"/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/guesswho_fakes/\"\n",
    "    else:\n",
    "        return \"//home/logan/SynologyDrive/Research/guesswho_new/guesswho18/data/deepfake_data/guesswho_fakes/\"\n",
    "\n",
    "def data_path_test():\n",
    "    sys_name = os.uname()[1]\n",
    "    if sys_name == 'Hive':\n",
    "        return \"/home/logan/drive/Research/Data_Stores/Timit/Data/timit/TIMIT/TEST\"\n",
    "    else:\n",
    "        return \"/home/logan/SynologyDrive/Research/Data_Stores/Timit/Data/timit/TIMIT/TEST\"\n",
    "\n",
    "def append_file(root, file, hive=False, add_slash=False):\n",
    "    if hive:\n",
    "        mod_root = \"E:\\\\SynologyDrive\\\\\" + root[root.find('Research'):]\n",
    "        if add_slash:\n",
    "            output = mod_root  + '\\\\' + file\n",
    "        else:\n",
    "            output = mod_root  + file\n",
    "        output =  output.replace('/', '\\\\')\n",
    "    else:\n",
    "        if add_slash:\n",
    "            output = root + '/' + file\n",
    "        else:\n",
    "            output = root + file\n",
    "    return output\n",
    "\n",
    "def cut_speaker_id(path):\n",
    "    return path[path.rfind('/') + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arpabet to ipa conversion\n",
    "arpa_key = ['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'axr', 'ay', 'eh', 'er', 'ey', 'ih', 'ix', 'iy', \\\n",
    "                'ow', 'oy', 'uh', 'uw', 'ux' , 'b', 'ch', 'd', 'dh', 'dx', 'el', 'em', 'en', 'f', \\\n",
    "                'g', 'h', 'hh', 'jh', 'k', 'l', 'm', 'n', 'ng', 'nx', 'p', 'q', 'r', 's', 'sh', \\\n",
    "                't', 'th', 'v', 'w', 'wh', 'y', 'z', 'zh', 'ax-h', 'bcl', 'dcl', 'eng', 'gcl', 'hv', \\\n",
    "                'kcl', 'pcl', 'tcl', 'pau', 'epi', 'h#']\n",
    "ipa_key = ['ɑ', 'æ', 'ʌ', 'ɔ', 'aʊ', 'ə', 'ɚ', 'aɪ', 'ɛ', 'ɝ', 'eɪ', 'ɪ', 'ɨ', 'i', 'oʊ', 'ɔɪ', \\\n",
    "                'ʊ', 'u', 'ʉ', 'b', 'tʃ', 'd', 'ð', 'ɾ', 'l̩', 'm̩', 'n̩', 'f', 'ɡ', 'h', 'h', 'dʒ', 'k', \\\n",
    "                'l', 'm', 'n', 'ŋ', 'ɾ̃', 'p', 'ʔ', 'ɹ', 's', 'ʃ', 't', 'θ', 'v', 'w', 'ʍ', 'j', 'z', \\\n",
    "                'ʒ', 'ə̥', 'b̚', 'd̚', 'ŋ̍', 'ɡ̚', 'ɦ', 'k̚', 'p̚', 't̚', 'N/A', 'N/A', 'N/A']\n",
    "\n",
    "ipa_conversion = dict(zip(arpa_key, ipa_key))\n",
    "def convert_to_ipa(arpa_key):\n",
    "    output = []\n",
    "    for key in arpa_key:\n",
    "        output.append(ipa_conversion[key])\n",
    "    return output\n",
    "\n",
    "arpa_conversion = dict(zip(ipa_key, arpa_key))\n",
    "def convert_from_ipa(ipa_key):\n",
    "    output = []\n",
    "    for key in ipa_key:\n",
    "        output.append(arpa_conversion.get(key, 'N/A'))\n",
    "    return output\n",
    "\n",
    "def join_word_phoneme(df_wrd, df_phn, audio_file):\n",
    "    new_df = df_wrd.merge(df_phn, 'outer', on=('sample_id', 'speaker_id'), \\\n",
    "                          suffixes=('_word', '_phoneme'))\n",
    "    new_df = new_df[(new_df.start_phoneme >= new_df.start_word) &\n",
    "                    (new_df.end_phoneme <= new_df.end_word)]\n",
    "    try:\n",
    "        new_df['ipa'] = convert_to_ipa(new_df['arpabet'])\n",
    "        new_df['filepath'] = audio_file\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "    if audio_file[audio_file.rfind('/') - 5] == 'M':\n",
    "        new_df['sex'] = 'm'\n",
    "    else:\n",
    "        new_df['sex'] = 'f'\n",
    "    return new_df\n",
    "\n",
    "def ipa_join_word_phoneme(df_wrd, df_phn, audio_file):\n",
    "    new_df = df_wrd.merge(df_phn, 'outer', on=('sample_id', 'speaker_id'), \\\n",
    "                          suffixes=('_word', '_phoneme'))\n",
    "    new_df = new_df[(new_df.start_phoneme >= new_df.start_word) &\n",
    "                    (new_df.end_phoneme <= new_df.end_word)]\n",
    "    try:\n",
    "        new_df['arpabet'] = convert_from_ipa(new_df['ipa'])\n",
    "        new_df['filepath'] = audio_file\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "    if audio_file[audio_file.rfind('/') - 5] == 'M':\n",
    "        new_df['sex'] = 'm'\n",
    "    else:\n",
    "        new_df['sex'] = 'f'\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyrebird Master Creation\n",
    "* Feb 2 2021 (For Usenix submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 2/2 [00:00<00:00, 18893.26it/s]\n",
      "100%|██████████████████████████████████████████████████████| 28/28 [00:00<00:00, 304.65it/s]\n",
      "100%|██████████████████████████████████████████████████████| 41/41 [00:00<00:00, 269.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_train = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', 'start_phoneme', \n",
    "        'end_phoneme', 'sex', 'arpabet', 'ipa', 'filepath'])\n",
    "for root, dirs, files in list(os.walk(data_path_lyre_true_extend())):     #training data\n",
    "    for file in tqdm(files, position=0, leave=True):\n",
    "        if 'wav' in file.lower():\n",
    "            last_under = file.rfind('_')\n",
    "            sample_id = file[last_under + 1:-4]\n",
    "            speaker_id = file[last_under - 5:last_under]\n",
    "            #get phoneme infor\n",
    "            phn_file = append_file(root, file[:-4]+'.PHN', add_slash=True)\n",
    "            df_phn = pd.read_csv(phn_file, delimiter=' ', names=['start', 'end', 'ipa'])\n",
    "            df_phn['sample_id'] = sample_id\n",
    "            df_phn['speaker_id'] = speaker_id\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, file[:-4]+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            df_train = df_train.append(ipa_join_word_phoneme(df_wrd, df_phn, \n",
    "                                    append_file(root, file, hive=True, add_slash=True)), ignore_index=True)\n",
    "\n",
    "#add phoneme indices for bigram analysis\n",
    "df_train['index_phoneme'] = -1\n",
    "grouped_df = df_train.groupby(['speaker_id', 'sample_id', 'start_word'])\n",
    "for key, item in grouped_df:\n",
    "    new_indices = list(range(len(item.index_phoneme)))\n",
    "    df_train.loc[item.index, 'index_phoneme'] = new_indices\n",
    "    \n",
    "#filter out unknown ipa values (epi, silence)\n",
    "all_ph = ['ʃ', 'ɨ', 'ɦ', 'ɛ', 'd̚', 'dʒ', 'ɪ', 'd', 'ʌ', 'k̚', 'k', 's', 'ʉ',\n",
    "       'ʔ', 'n̩', 'ɡ̚', 'ɡ', 'ɹ', 'w', 'ɔ', 'ɾ', 'ɚ', 'l', 'j', 'ʊ', 'n',\n",
    "       'æ', 'm', 'ɔɪ', 'ə', 'ð', 't̚', 'i', 'v', 'f', 't', 'p̚', 'oʊ',\n",
    "       'h', 'tʃ', 'b̚', 'b', 'ɑ', 'm̩', 'ŋ', 'aɪ', 'θ', 'ə̥', 'eɪ',\n",
    "       'p', 'aʊ', 'ɝ', 'ɾ̃', 'z', 'l̩', 'u', 'ʒ', 'ŋ̍']\n",
    "\n",
    "df_train = df_train[df_train.ipa.isin(all_ph)]\n",
    "df_train.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_train.to_csv('../../data/lyrebird_true_usenix_master.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████| 16/16 [00:00<00:00, 269.59it/s]\n",
      "100%|██████████████████████████████████████████████████████| 20/20 [00:00<00:00, 282.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_train = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', 'start_phoneme', \n",
    "        'end_phoneme', 'sex', 'arpabet', 'ipa', 'filepath'])\n",
    "for root, dirs, files in list(os.walk(data_path_lyre_fake_extend())):     #training data\n",
    "    for file in tqdm(files, position=0, leave=True):\n",
    "        if 'wav' in file.lower():\n",
    "            last_under = file.rfind('_')\n",
    "            sample_id = file[last_under + 1:-4]\n",
    "            speaker_id = file[last_under - 5:last_under]\n",
    "            #get phoneme infor\n",
    "            phn_file = append_file(root, file[:-4]+'.PHN', add_slash=True)\n",
    "            df_phn = pd.read_csv(phn_file, delimiter=' ', names=['start', 'end', 'ipa'])\n",
    "            df_phn['sample_id'] = sample_id\n",
    "            df_phn['speaker_id'] = speaker_id\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, file[:-4]+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            df_train = df_train.append(ipa_join_word_phoneme(df_wrd, df_phn, \n",
    "                                    append_file(root, file, hive=True, add_slash=True)), ignore_index=True)\n",
    "\n",
    "#add phoneme indices for bigram analysis\n",
    "df_train['index_phoneme'] = -1\n",
    "grouped_df = df_train.groupby(['speaker_id', 'sample_id', 'start_word'])\n",
    "for key, item in grouped_df:\n",
    "    new_indices = list(range(len(item.index_phoneme)))\n",
    "    df_train.loc[item.index, 'index_phoneme'] = new_indices\n",
    "    \n",
    "#filter out unknown ipa values (epi, silence)\n",
    "all_ph = ['ʃ', 'ɨ', 'ɦ', 'ɛ', 'd̚', 'dʒ', 'ɪ', 'd', 'ʌ', 'k̚', 'k', 's', 'ʉ',\n",
    "       'ʔ', 'n̩', 'ɡ̚', 'ɡ', 'ɹ', 'w', 'ɔ', 'ɾ', 'ɚ', 'l', 'j', 'ʊ', 'n',\n",
    "       'æ', 'm', 'ɔɪ', 'ə', 'ð', 't̚', 'i', 'v', 'f', 't', 'p̚', 'oʊ',\n",
    "       'h', 'tʃ', 'b̚', 'b', 'ɑ', 'm̩', 'ŋ', 'aɪ', 'θ', 'ə̥', 'eɪ',\n",
    "       'p', 'aʊ', 'ɝ', 'ɾ̃', 'z', 'l̩', 'u', 'ʒ', 'ŋ̍']\n",
    "\n",
    "df_train = df_train[df_train.ipa.isin(all_ph)]\n",
    "df_train.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_train.to_csv('../../data/lyrebird_fake_usenix_master.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/logan/SynologyDrive/Research/guesswho_new/guesswho18/data/deepfake_data/lyre_bird_comp/real/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_lyre_true_extend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timit master creation\n",
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#step through all directories in data path, consume all files with s^Y vimame name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_train = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', 'start_phoneme', \n",
    "        'end_phoneme', 'sex', 'arpabet', 'ipa', 'filepath'])\n",
    "for root, dirs, files in list(os.walk(data_path_train())):     #training data\n",
    "    for file in tqdm(files, position=0, leave=True):\n",
    "        if 'wav' in file.lower():\n",
    "            last_under = file.rfind('_')\n",
    "            sample_id = file[last_under + 1:-4]\n",
    "            speaker_id = file[last_under - 5:last_under]\n",
    "            #get phoneme infor\n",
    "            phn_file = append_file(root, file[:-4]+'.PHN', add_slash=True)\n",
    "            df_phn = pd.read_csv(phn_file, delimiter=' ', names=['start', 'end', 'ipa'])\n",
    "            df_phn['sample_id'] = sample_id\n",
    "            df_phn['speaker_id'] = speaker_id\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, file[:-4]+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            df_train = df_train.append(ipa_join_word_phoneme(df_wrd, df_phn, \n",
    "                                    append_file(root, file, hive=True, add_slash=True)), ignore_index=True)\n",
    "\n",
    "#add phoneme indices for bigram analysis\n",
    "df_train['index_phoneme'] = -1\n",
    "grouped_df = df_train.groupby(['speaker_id', 'sample_id', 'start_word'])\n",
    "for key, item in grouped_df:\n",
    "    new_indices = list(range(len(item.index_phoneme)))\n",
    "    df_train.loc[item.index, 'index_phoneme'] = new_indices\n",
    "    \n",
    "#filter out unknown ipa values (epi, silence)\n",
    "all_ph = ['ʃ', 'ɨ', 'ɦ', 'ɛ', 'd̚', 'dʒ', 'ɪ', 'd', 'ʌ', 'k̚', 'k', 's', 'ʉ',\n",
    "       'ʔ', 'n̩', 'ɡ̚', 'ɡ', 'ɹ', 'w', 'ɔ', 'ɾ', 'ɚ', 'l', 'j', 'ʊ', 'n',\n",
    "       'æ', 'm', 'ɔɪ', 'ə', 'ð', 't̚', 'i', 'v', 'f', 't', 'p̚', 'oʊ',\n",
    "       'h', 'tʃ', 'b̚', 'b', 'ɑ', 'm̩', 'ŋ', 'aɪ', 'θ', 'ə̥', 'eɪ',\n",
    "       'p', 'aʊ', 'ɝ', 'ɾ̃', 'z', 'l̩', 'u', 'ʒ', 'ŋ̍']\n",
    "\n",
    "df_train = df_train[df_train.ipa.isin(all_ph)]\n",
    "df_train.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_train.to_csv('../../data/timit_master.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_test = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', 'start_phoneme', \n",
    "        'end_phoneme', 'sex', 'arpabet', 'ipa', 'filepath'])\n",
    "for root, dirs, files in tqdm(list(os.walk(data_path_test()))):     #training data\n",
    "    for file in files:\n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            speaker_id = cut_speaker_id(root)\n",
    "            #get phoneme infor\n",
    "            phn_file = append_file(root, sample_id+'.PHN', add_slash=True)\n",
    "            df_phn = pd.read_csv(phn_file, delimiter=' ', names=['start', 'end', 'arpabet'])\n",
    "            df_phn['sample_id'] = sample_id\n",
    "            df_phn['speaker_id'] = speaker_id\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            df_test = df_test.append(join_word_phoneme(df_wrd, df_phn, \n",
    "                                    append_file(root, file, hive=True, add_slash=True)), ignore_index=True)\n",
    "\n",
    "#add phoneme indices for bigram analysis\n",
    "df_test['index_phoneme'] = -1\n",
    "grouped_df = df_test.groupby(['speaker_id', 'sample_id', 'start_word'])\n",
    "for key, item in grouped_df:\n",
    "    new_indices = list(range(len(item.index_phoneme)))\n",
    "    df_test.loc[item.index, 'index_phoneme'] = new_indices\n",
    "    \n",
    "#filter out unknown ipa values (epi, silence)\n",
    "all_ph = ['ʃ', 'ɨ', 'ɦ', 'ɛ', 'd̚', 'dʒ', 'ɪ', 'd', 'ʌ', 'k̚', 'k', 's', 'ʉ',\n",
    "       'ʔ', 'n̩', 'ɡ̚', 'ɡ', 'ɹ', 'w', 'ɔ', 'ɾ', 'ɚ', 'l', 'j', 'ʊ', 'n',\n",
    "       'æ', 'm', 'ɔɪ', 'ə', 'ð', 't̚', 'i', 'v', 'f', 't', 'p̚', 'oʊ',\n",
    "       'h', 'tʃ', 'b̚', 'b', 'ɑ', 'm̩', 'ŋ', 'aɪ', 'θ', 'ə̥', 'eɪ',\n",
    "       'p', 'aʊ', 'ɝ', 'ɾ̃', 'z', 'l̩', 'u', 'ʒ', 'ŋ̍']\n",
    "\n",
    "df_test = df_test[df_test.ipa.isin(all_ph)]\n",
    "df_test.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "#df_test.to_csv('../../data/timit_test_master.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_word_x</th>\n",
       "      <th>end_word_x</th>\n",
       "      <th>word_x</th>\n",
       "      <th>sample_id_x</th>\n",
       "      <th>start_phoneme_x</th>\n",
       "      <th>end_phoneme_x</th>\n",
       "      <th>sex_x</th>\n",
       "      <th>arpabet_x</th>\n",
       "      <th>ipa_x</th>\n",
       "      <th>filepath_x</th>\n",
       "      <th>...</th>\n",
       "      <th>word_y</th>\n",
       "      <th>sample_id_y</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>start_phoneme_y</th>\n",
       "      <th>end_phoneme_y</th>\n",
       "      <th>sex_y</th>\n",
       "      <th>arpabet_y</th>\n",
       "      <th>ipa_y</th>\n",
       "      <th>filepath_y</th>\n",
       "      <th>index_phoneme_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [start_word_x, end_word_x, word_x, sample_id_x, start_phoneme_x, end_phoneme_x, sex_x, arpabet_x, ipa_x, filepath_x, index_phoneme_x, start_word_y, end_word_y, word_y, sample_id_y, speaker_id, start_phoneme_y, end_phoneme_y, sex_y, arpabet_y, ipa_y, filepath_y, index_phoneme_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.merge(df_train, left_on='speaker_id', right_on='speaker_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake master creation\n",
    "* First cell is for fully labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This second cell is for word labels only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len:  0\n"
     ]
    }
   ],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_timit = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', \\\n",
    "             'sex', 'filepath'])\n",
    "\n",
    "for root, _, files in os.walk(data_path_fakes()):     #deep fakes data\n",
    "    for file in files:        \n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            speaker_id = file[16:21]\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD')\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start_word', 'end_word', 'word'])            \n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            fpath = append_file(root, file, hive=True)\n",
    "            df_wrd['filepath'] = fpath\n",
    "            if fpath[fpath[:fpath.rfind('_')].rfind('_') - 5] == 'M':\n",
    "                df_wrd['sex'] = 'm'\n",
    "            else:\n",
    "                df_wrd['sex'] = 'f'\n",
    "            df_timit = df_timit.append(df_wrd, ignore_index=True)\n",
    "print(\"Len: \", df_timit.count()[0])\n",
    "\n",
    "df_timit.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_timit.to_csv('../../data/real_time_master.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gentle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_test = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', 'start_phoneme', \n",
    "        'end_phoneme', 'sex', 'arpabet', 'ipa', 'filepath'])\n",
    "\n",
    "data_path = '/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/guesswho_fakes'\n",
    "for root, dirs, files in tqdm(list(os.walk(data_path)), position=0, leave=True):     #training data\n",
    "    for file in files:\n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            speaker_id = file[16:21]            \n",
    "                \n",
    "            #get phoneme infor\n",
    "            phn_file = append_file(root, sample_id+'.PHN', add_slash=True)\n",
    "            df_phn = pd.read_csv(phn_file, delimiter=' ', names=['start', 'end', 'ipa'])\n",
    "            df_phn['sample_id'] = sample_id\n",
    "            df_phn['speaker_id'] = speaker_id\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            df_test = df_test.append(ipa_join_word_phoneme(df_wrd, df_phn, \n",
    "                                    append_file(root, file, hive=True, add_slash=True)), ignore_index=True)\n",
    "\n",
    "\n",
    "df_raw = df_test\n",
    "#add phoneme indices for bigram analysis\n",
    "df_test['index_phoneme'] = -1\n",
    "grouped_df = df_test.groupby(['speaker_id', 'sample_id', 'start_word'])\n",
    "for key, item in grouped_df:\n",
    "    new_indices = list(range(len(item.index_phoneme)))\n",
    "    df_test.loc[item.index, 'index_phoneme'] = new_indices\n",
    "    \n",
    "#filter out unknown ipa values (epi, silence)\n",
    "all_ph = ['ʃ', 'ɨ', 'ɦ', 'ɛ', 'd̚', 'dʒ', 'ɪ', 'd', 'ʌ', 'k̚', 'k', 's', 'ʉ',\n",
    "       'ʔ', 'n̩', 'ɡ̚', 'ɡ', 'ɹ', 'w', 'ɔ', 'ɾ', 'ɚ', 'l', 'j', 'ʊ', 'n',\n",
    "       'æ', 'm', 'ɔɪ', 'ə', 'ð', 't̚', 'i', 'v', 'f', 't', 'p̚', 'oʊ',\n",
    "       'h', 'tʃ', 'b̚', 'b', 'ɑ', 'm̩', 'ŋ', 'aɪ', 'θ', 'ə̥', 'eɪ',\n",
    "       'p', 'aʊ', 'ɝ', 'ɾ̃', 'z', 'l̩', 'u', 'ʒ', 'ŋ̍']\n",
    "\n",
    "df_test = df_test[df_test.ipa.isin(all_ph)]\n",
    "df_test.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_test.to_csv('../../data/real_time_gentle_master.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('../../data/real_time_gentle_master.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_word</th>\n",
       "      <th>end_word</th>\n",
       "      <th>word</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>start_phoneme</th>\n",
       "      <th>end_phoneme</th>\n",
       "      <th>sex</th>\n",
       "      <th>arpabet</th>\n",
       "      <th>ipa</th>\n",
       "      <th>filepath</th>\n",
       "      <th>index_phoneme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [start_word, end_word, word, sample_id, speaker_id, start_phoneme, end_phoneme, sex, arpabet, ipa, filepath, index_phoneme]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrebird Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (748468952.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_7156/748468952.py\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    for root, dirs, files in tqdm(list(os.walk(data_path_test)):     #training data\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_test = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', 'start_phoneme', \n",
    "        'end_phoneme', 'sex', 'arpabet', 'ipa', 'filepath'])\n",
    "\n",
    "data_path = '/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/lyre_bird/true/'\n",
    "for root, dirs, files in tqdm(list(os.walk(data_path_test)):     #training data\n",
    "    for file in files:\n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            if 'trump' in file:\n",
    "                speaker_id = 'trump'\n",
    "            else:\n",
    "                speaker_id = 'obama'\n",
    "                \n",
    "            #get phoneme infor\n",
    "            phn_file = append_file(root, sample_id+'.PHN', add_slash=True)\n",
    "            df_phn = pd.read_csv(phn_file, delimiter=' ', names=['start', 'end', 'arpabet'])\n",
    "            df_phn['sample_id'] = sample_id\n",
    "            df_phn['speaker_id'] = speaker_id\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            df_test = df_test.append(join_word_phoneme(df_wrd, df_phn, \n",
    "                                    append_file(root, file, hive=True, add_slash=True)), ignore_index=True)\n",
    "\n",
    "#add phoneme indices for bigram analysis\n",
    "df_test['index_phoneme'] = -1\n",
    "grouped_df = df_test.groupby(['speaker_id', 'sample_id', 'start_word'])\n",
    "for key, item in grouped_df:\n",
    "    new_indices = list(range(len(item.index_phoneme)))\n",
    "    df_test.loc[item.index, 'index_phoneme'] = new_indices\n",
    "    \n",
    "#filter out unknown ipa values (epi, silence)\n",
    "all_ph = ['ʃ', 'ɨ', 'ɦ', 'ɛ', 'd̚', 'dʒ', 'ɪ', 'd', 'ʌ', 'k̚', 'k', 's', 'ʉ',\n",
    "       'ʔ', 'n̩', 'ɡ̚', 'ɡ', 'ɹ', 'w', 'ɔ', 'ɾ', 'ɚ', 'l', 'j', 'ʊ', 'n',\n",
    "       'æ', 'm', 'ɔɪ', 'ə', 'ð', 't̚', 'i', 'v', 'f', 't', 'p̚', 'oʊ',\n",
    "       'h', 'tʃ', 'b̚', 'b', 'ɑ', 'm̩', 'ŋ', 'aɪ', 'θ', 'ə̥', 'eɪ',\n",
    "       'p', 'aʊ', 'ɝ', 'ɾ̃', 'z', 'l̩', 'u', 'ʒ', 'ŋ̍']\n",
    "\n",
    "df_test = df_test[df_test.ipa.isin(all_ph)]\n",
    "df_test.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_timit.to_csv('../../data/lyre_bird_true_master.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_timit = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', \\\n",
    "             'sex', 'filepath'])\n",
    "data_path = '/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/lyre_bird/real/'\n",
    "for root, _, files in tqdm(list(os.walk(data_path))):     #deep fakes data\n",
    "    for file in files:        \n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            if 'trump' in file:\n",
    "                speaker_id = 'trump'\n",
    "            else:\n",
    "                speaker_id = 'obama'\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start_word', 'end_word', 'word'])            \n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            fpath = append_file(root, file, hive=True, add_slash=True)\n",
    "            df_wrd['filepath'] = fpath\n",
    "            if fpath[fpath[:fpath.rfind('_')].rfind('_') - 5] == 'M':\n",
    "                df_wrd['sex'] = 'm'\n",
    "            else:\n",
    "                df_wrd['sex'] = 'f'\n",
    "            df_timit = df_timit.append(df_wrd, ignore_index=True)\n",
    "print(\"Len: \", df_timit.count()[0])\n",
    "\n",
    "df_timit.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_timit.to_csv('../../data/lyre_bird_true_master.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_timit = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', \\\n",
    "             'sex', 'filepath'])\n",
    "data_path = '/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/lyre_bird/fake/'\n",
    "for root, _, files in tqdm(list(os.walk(data_path))):     #deep fakes data\n",
    "    for file in files:        \n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            if 'trump' in file:\n",
    "                speaker_id = 'trump'\n",
    "            else:\n",
    "                speaker_id = 'obama'\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start_word', 'end_word', 'word'])            \n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            fpath = append_file(root, file, hive=True, add_slash=True)\n",
    "            df_wrd['filepath'] = fpath\n",
    "            if fpath[fpath[:fpath.rfind('_')].rfind('_') - 5] == 'M':\n",
    "                df_wrd['sex'] = 'm'\n",
    "            else:\n",
    "                df_wrd['sex'] = 'f'\n",
    "            df_timit = df_timit.append(df_wrd, ignore_index=True)\n",
    "print(\"Len: \", df_timit.count()[0])\n",
    "\n",
    "df_timit.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_timit.to_csv('../../data/lyre_bird_fake_master.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASV Spoof\n",
    "\n",
    "## True set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_test = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', 'start_phoneme', \n",
    "        'end_phoneme', 'sex', 'arpabet', 'ipa', 'filepath'])\n",
    "\n",
    "data_path = '/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/ASV_spoof/real/'\n",
    "for root, dirs, files in tqdm(list(os.walk(data_path)), position=0, leave=True):     #training data\n",
    "    for file in files:\n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            speaker_id = root[-16:-9]            \n",
    "                \n",
    "            #get phoneme infor\n",
    "            phn_file = append_file(root, sample_id+'.PHN', add_slash=True)\n",
    "            df_phn = pd.read_csv(phn_file, delimiter=' ', names=['start', 'end', 'ipa'])\n",
    "            df_phn['sample_id'] = sample_id\n",
    "            df_phn['speaker_id'] = speaker_id\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            df_test = df_test.append(ipa_join_word_phoneme(df_wrd, df_phn, \n",
    "                                    append_file(root, file, hive=True, add_slash=True)), ignore_index=True)\n",
    "\n",
    "#add phoneme indices for bigram analysis\n",
    "df_test['index_phoneme'] = -1\n",
    "grouped_df = df_test.groupby(['speaker_id', 'sample_id', 'start_word'])\n",
    "for key, item in grouped_df:\n",
    "    new_indices = list(range(len(item.index_phoneme)))\n",
    "    df_test.loc[item.index, 'index_phoneme'] = new_indices\n",
    "    \n",
    "#filter out unknown ipa values (epi, silence)\n",
    "all_ph = ['ʃ', 'ɨ', 'ɦ', 'ɛ', 'd̚', 'dʒ', 'ɪ', 'd', 'ʌ', 'k̚', 'k', 's', 'ʉ',\n",
    "       'ʔ', 'n̩', 'ɡ̚', 'ɡ', 'ɹ', 'w', 'ɔ', 'ɾ', 'ɚ', 'l', 'j', 'ʊ', 'n',\n",
    "       'æ', 'm', 'ɔɪ', 'ə', 'ð', 't̚', 'i', 'v', 'f', 't', 'p̚', 'oʊ',\n",
    "       'h', 'tʃ', 'b̚', 'b', 'ɑ', 'm̩', 'ŋ', 'aɪ', 'θ', 'ə̥', 'eɪ',\n",
    "       'p', 'aʊ', 'ɝ', 'ɾ̃', 'z', 'l̩', 'u', 'ʒ', 'ŋ̍']\n",
    "\n",
    "df_test = df_test[df_test.ipa.isin(all_ph)]\n",
    "df_test.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_test.to_csv('../../data/asv_spoof_true_master.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_test = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', 'start_phoneme', \n",
    "        'end_phoneme', 'sex', 'arpabet', 'ipa', 'filepath'])\n",
    "\n",
    "data_path = '/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/ASV_spoof/fake/'\n",
    "for root, dirs, files in tqdm(list(os.walk(data_path)), position=0, leave=True):     #training data\n",
    "    for file in files:\n",
    "        if 'wav' in file.lower():\n",
    "            try:\n",
    "                sample_id = file[:-4]\n",
    "                speaker_id = root[-16:-9]            \n",
    "\n",
    "                #get phoneme infor\n",
    "                phn_file = append_file(root, sample_id+'.PHN', add_slash=True)\n",
    "                df_phn = pd.read_csv(phn_file, delimiter=' ', names=['start', 'end', 'ipa'])\n",
    "                df_phn['sample_id'] = sample_id\n",
    "                df_phn['speaker_id'] = speaker_id\n",
    "\n",
    "                #get word info\n",
    "                wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "                df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "                df_wrd['sample_id'] = sample_id\n",
    "                df_wrd['speaker_id'] = speaker_id\n",
    "                df_test = df_test.append(ipa_join_word_phoneme(df_wrd, df_phn, \n",
    "                                    append_file(root, file, hive=True, add_slash=True)), ignore_index=True)\n",
    "            except:\n",
    "                #likely that wrd or phn file not created, just skip\n",
    "                pass\n",
    "\n",
    "#add phoneme indices for bigram analysis\n",
    "df_test['index_phoneme'] = -1\n",
    "grouped_df = df_test.groupby(['speaker_id', 'sample_id', 'start_word'])\n",
    "for key, item in grouped_df:\n",
    "    new_indices = list(range(len(item.index_phoneme)))\n",
    "    df_test.loc[item.index, 'index_phoneme'] = new_indices\n",
    "    \n",
    "#filter out unknown ipa values (epi, silence)\n",
    "all_ph = ['ʃ', 'ɨ', 'ɦ', 'ɛ', 'd̚', 'dʒ', 'ɪ', 'd', 'ʌ', 'k̚', 'k', 's', 'ʉ',\n",
    "       'ʔ', 'n̩', 'ɡ̚', 'ɡ', 'ɹ', 'w', 'ɔ', 'ɾ', 'ɚ', 'l', 'j', 'ʊ', 'n',\n",
    "       'æ', 'm', 'ɔɪ', 'ə', 'ð', 't̚', 'i', 'v', 'f', 't', 'p̚', 'oʊ',\n",
    "       'h', 'tʃ', 'b̚', 'b', 'ɑ', 'm̩', 'ŋ', 'aɪ', 'θ', 'ə̥', 'eɪ',\n",
    "       'p', 'aʊ', 'ɝ', 'ɾ̃', 'z', 'l̩', 'u', 'ʒ', 'ŋ̍']\n",
    "\n",
    "df_test = df_test[df_test.ipa.isin(all_ph)]\n",
    "df_test.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_test.to_csv('../../data/asv_spoof_gentle_master.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"============Old==============================\"\"\"\n",
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_timit = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', \\\n",
    "             'sex', 'filepath'])\n",
    "data_path = '/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/lyre_bird/fake/'\n",
    "for root, _, files in tqdm(list(os.walk(data_path))):     #deep fakes data\n",
    "    for file in files:        \n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            speaker_id = root[-16:-9] \n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start_word', 'end_word', 'word'])            \n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            fpath = append_file(root, file, hive=True, add_slash=True)\n",
    "            df_wrd['filepath'] = fpath\n",
    "            df_wrd['sex'] = 'f'\n",
    "\n",
    "            df_timit = df_timit.append(df_wrd, ignore_index=True)\n",
    "print(\"Len: \", df_timit.count()[0])\n",
    "\n",
    "df_timit.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_timit.to_csv('../../data/asv_spoof_fake_master.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create consistency validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_dir = '/home/logan/drive/Research/guesswho18/'     #hive\n",
    "#gw_dir = '/User/logan/Gogle_Drive/Research/guesswho18/'     #iMac\n",
    "\n",
    "df_timit_read = pd.read_csv(gw_dir + 'data/timit_master.csv', sep=',',\n",
    "       dtype = {\n",
    "           'start_word' : np.int,\n",
    "           'end_word': np.int,\n",
    "           'word': np.str,\n",
    "           'sample_id': np.str,\n",
    "           'speaker_id': np.str,\n",
    "           'start_phoneme': np.int,\n",
    "           'end_phoneme': np.int,\n",
    "           'arpabet': np.str,\n",
    "           'ipa': str,\n",
    "           'filename': np.str,\n",
    "           'index_phoneme': np.int\n",
    "       })\n",
    "print(\"Timit done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = list(df_timit_read['filepath'].unique())\n",
    "import random\n",
    "random.seed(13)\n",
    "sampled_paths = random.sample(paths, 490)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_timit_read[df_timit_read.filepath.isin(sampled_paths)]\n",
    "df_sample = df_sample.reset_index()\n",
    "df_sample.drop(columns=['index'])\n",
    "df_sample.to_csv('../../data/consistent_master.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
