{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a746de-ef56-4c13-85a6-66b2d878e741",
   "metadata": {
    "tags": []
   },
   "source": [
    "# This notebook will create the df_timit and save it as a csv for later use. \n",
    "    - We need to step through all the directories in the timit data store and create necessary rows\n",
    "    - Also need to augment the data with some additional info\n",
    "    \n",
    "## Dialect information\n",
    "     dr1:  New England\n",
    "     dr2:  Northern\n",
    "     dr3:  North Midland\n",
    "     dr4:  South Midland\n",
    "     dr5:  Southern\n",
    "     dr6:  New York City\n",
    "     dr7:  Western\n",
    "     dr8:  Army Brat (moved around)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fda8e849-277b-43f8-8394-1f72be8a4590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "#increase some pandas vars\n",
    "pd.set_option('max_colwidth', 100)\n",
    "sys_name = os.uname()[1]\n",
    "\n",
    "def data_path():    \n",
    "    return \"/home/logan/Downloads/CI_simmed_processed/CI_simed\"\n",
    "        \n",
    "\n",
    "def append_file(root, file, hive=False, add_slash=False):\n",
    "    if hive:\n",
    "        mod_root = \"E:\\\\SynologyDrive\\\\\" + root[root.find('Research'):]\n",
    "        if add_slash:\n",
    "            output = mod_root  + '\\\\' + file\n",
    "        else:\n",
    "            output = mod_root  + file\n",
    "        output =  output.replace('/', '\\\\')\n",
    "    else:\n",
    "        if add_slash:\n",
    "            output = root + '/' + file\n",
    "        else:\n",
    "            output = root + file\n",
    "    return output\n",
    "\n",
    "def cut_speaker_id(path):\n",
    "    return path[path.rfind('/') + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fbe0b8c-dd4e-499c-ae5e-6312d3c72ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arpabet to ipa conversion\n",
    "arpa_key = ['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'axr', 'ay', 'eh', 'er', 'ey', 'ih', 'ix', 'iy', \\\n",
    "                'ow', 'oy', 'uh', 'uw', 'ux' , 'b', 'ch', 'd', 'dh', 'dx', 'el', 'em', 'en', 'f', \\\n",
    "                'g', 'h', 'hh', 'jh', 'k', 'l', 'm', 'n', 'ng', 'nx', 'p', 'q', 'r', 's', 'sh', \\\n",
    "                't', 'th', 'v', 'w', 'wh', 'y', 'z', 'zh', 'ax-h', 'bcl', 'dcl', 'eng', 'gcl', 'hv', \\\n",
    "                'kcl', 'pcl', 'tcl', 'pau', 'epi', 'h#']\n",
    "ipa_key = ['ɑ', 'æ', 'ʌ', 'ɔ', 'aʊ', 'ə', 'ɚ', 'aɪ', 'ɛ', 'ɝ', 'eɪ', 'ɪ', 'ɨ', 'i', 'oʊ', 'ɔɪ', \\\n",
    "                'ʊ', 'u', 'ʉ', 'b', 'tʃ', 'd', 'ð', 'ɾ', 'l̩', 'm̩', 'n̩', 'f', 'ɡ', 'h', 'h', 'dʒ', 'k', \\\n",
    "                'l', 'm', 'n', 'ŋ', 'ɾ̃', 'p', 'ʔ', 'ɹ', 's', 'ʃ', 't', 'θ', 'v', 'w', 'ʍ', 'j', 'z', \\\n",
    "                'ʒ', 'ə̥', 'b̚', 'd̚', 'ŋ̍', 'ɡ̚', 'ɦ', 'k̚', 'p̚', 't̚', 'N/A', 'N/A', 'N/A']\n",
    "\n",
    "ipa_conversion = dict(zip(arpa_key, ipa_key))\n",
    "def convert_to_ipa(arpa_key):\n",
    "    output = []\n",
    "    for key in arpa_key:\n",
    "        output.append(ipa_conversion[key])\n",
    "    return output\n",
    "\n",
    "arpa_conversion = dict(zip(ipa_key, arpa_key))\n",
    "def convert_from_ipa(ipa_key):\n",
    "    output = []\n",
    "    for key in ipa_key:\n",
    "        output.append(arpa_conversion.get(key, 'N/A'))\n",
    "    return output\n",
    "\n",
    "def join_word_phoneme(df_wrd, df_phn, audio_file):\n",
    "    new_df = df_wrd.merge(df_phn, 'outer', on=('sample_id', 'speaker_id'), \\\n",
    "                          suffixes=('_word', '_phoneme'))\n",
    "    new_df = new_df[(new_df.start_phoneme >= new_df.start_word) &\n",
    "                    (new_df.end_phoneme <= new_df.end_word)]\n",
    "    try:\n",
    "        new_df['ipa'] = convert_to_ipa(new_df['arpabet'])\n",
    "        new_df['filepath'] = audio_file\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "    if audio_file[audio_file.rfind('/') - 5] == 'M':\n",
    "        new_df['sex'] = 'm'\n",
    "    else:\n",
    "        new_df['sex'] = 'f'\n",
    "    return new_df\n",
    "\n",
    "def ipa_join_word_phoneme(df_wrd, df_phn, audio_file):\n",
    "    new_df = df_wrd.merge(df_phn, 'outer', on=('sample_id', 'speaker_id'), \\\n",
    "                          suffixes=('_word', '_phoneme'))\n",
    "    new_df = new_df[(new_df.start_phoneme >= new_df.start_word) &\n",
    "                    (new_df.end_phoneme <= new_df.end_word)]\n",
    "    try:\n",
    "        new_df['arpabet'] = convert_from_ipa(new_df['ipa'])\n",
    "        new_df['filepath'] = audio_file\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "    if audio_file[audio_file.rfind('/') - 5] == 'M':\n",
    "        new_df['sex'] = 'm'\n",
    "    else:\n",
    "        new_df['sex'] = 'f'\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db750498-14f5-47bd-8f0a-3146b0901108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 626/626 [00:02<00:00, 271.86it/s]\n",
      "100%|██████████| 263/263 [00:00<00:00, 1472766.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_train = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', 'start_phoneme', \n",
    "        'end_phoneme', 'sex', 'arpabet', 'ipa', 'filepath'])\n",
    "for root, dirs, files in list(os.walk(data_path())):     #training data\n",
    "    for file in tqdm(files, position=0, leave=True):\n",
    "        try:\n",
    "            if 'wav' in file.lower():\n",
    "                last_under = file.rfind('_')\n",
    "                sample_id = file[last_under + 1:-4]\n",
    "                speaker_id = file[last_under - 5:last_under]\n",
    "                #get phoneme infor\n",
    "                phn_file = append_file(root, file[:-4]+'.PHN', add_slash=True)\n",
    "                df_phn = pd.read_csv(phn_file, delimiter=' ', names=['start', 'end', 'ipa'])\n",
    "                df_phn['sample_id'] = sample_id\n",
    "                df_phn['speaker_id'] = speaker_id\n",
    "\n",
    "                #get word info\n",
    "                wrd_file = append_file(root, file[:-4]+'.WRD', add_slash=True)\n",
    "                df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "                df_wrd['sample_id'] = sample_id\n",
    "                df_wrd['speaker_id'] = speaker_id\n",
    "                df_train = df_train.append(ipa_join_word_phoneme(df_wrd, df_phn, \n",
    "                                        append_file(root, file, add_slash=True)), ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "#add phoneme indices for bigram analysis\n",
    "df_train['index_phoneme'] = -1\n",
    "grouped_df = df_train.groupby(['speaker_id', 'sample_id', 'start_word'])\n",
    "for key, item in grouped_df:\n",
    "    new_indices = list(range(len(item.index_phoneme)))\n",
    "    df_train.loc[item.index, 'index_phoneme'] = new_indices\n",
    "    \n",
    "#filter out unknown ipa values (epi, silence)\n",
    "all_ph = ['ʃ', 'ɨ', 'ɦ', 'ɛ', 'd̚', 'dʒ', 'ɪ', 'd', 'ʌ', 'k̚', 'k', 's', 'ʉ',\n",
    "       'ʔ', 'n̩', 'ɡ̚', 'ɡ', 'ɹ', 'w', 'ɔ', 'ɾ', 'ɚ', 'l', 'j', 'ʊ', 'n',\n",
    "       'æ', 'm', 'ɔɪ', 'ə', 'ð', 't̚', 'i', 'v', 'f', 't', 'p̚', 'oʊ',\n",
    "       'h', 'tʃ', 'b̚', 'b', 'ɑ', 'm̩', 'ŋ', 'aɪ', 'θ', 'ə̥', 'eɪ',\n",
    "       'p', 'aʊ', 'ɝ', 'ɾ̃', 'z', 'l̩', 'u', 'ʒ', 'ŋ̍']\n",
    "\n",
    "df_train = df_train[df_train.ipa.isin(all_ph)]\n",
    "df_train.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_train.to_csv('../../data/loc_master.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc551d45-792f-4fcb-ba6c-892a1067ea74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_3.8.0",
   "language": "python",
   "name": "gen_3.8.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
